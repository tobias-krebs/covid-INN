{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trying-patient",
   "metadata": {},
   "source": [
    "Notebook to replicate our results for data set 2 which uses a RNN for condition procession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aVx1D4_OuZ8H",
   "metadata": {
    "executionInfo": {
     "elapsed": 4675,
     "status": "ok",
     "timestamp": 1632071731651,
     "user": {
      "displayName": "Tobi Krebs",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10647988187130389750"
     },
     "user_tz": -120
    },
    "id": "aVx1D4_OuZ8H"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from FrEIA.framework import *\n",
    "from FrEIA.modules import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\")\n",
    "pad = lambda x: nn.utils.rnn.pad_sequence(x, batch_first = True).to(device)\n",
    "pack = lambda x, y: nn.utils.rnn.pack_padded_sequence(x, y, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yZD6tLtuZ8N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70253,
     "status": "ok",
     "timestamp": 1632071801903,
     "user": {
      "displayName": "Tobi Krebs",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10647988187130389750"
     },
     "user_tz": -120
    },
    "id": "7yZD6tLtuZ8N",
    "outputId": "e9921829-11ac-4767-aa74-a302bb78427f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Helper function to read the data which is saved in multiple .csv files\n",
    "    converts data to pytorch tensors\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_params = []\n",
    "    all_length = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(True):\n",
    "        name_dat = f\"./data_mobility{j}/dat{i}.csv\"\n",
    "        name_param = f\"./params_mobility{j}/params{i}.csv\"\n",
    "        try:\n",
    "            df_dat = pd.read_csv(name_dat)\n",
    "            df_param = pd.read_csv(name_param)\n",
    "            param = df_param.to_numpy()[0,[0,1,10]]\n",
    "            data = df_dat.iloc[:,2].to_numpy().reshape(-1,7)[:,[0,1,2,3]]\n",
    "            if len(data[np.isnan(data)]) == 0:\n",
    "                all_data.append(torch.from_numpy(data).to(torch.float32).to(device))\n",
    "                all_params.append(param)\n",
    "                all_length.append(len(data))\n",
    "            else:\n",
    "                print(i)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            i = 0\n",
    "            j += 1\n",
    "            print(\"Loaded\",len(all_data),\"Entries\")\n",
    "        if j>8:\n",
    "            print(\"not so much data\")\n",
    "            break\n",
    "        if len(all_data)%1000 == 0:\n",
    "            print(\"Loaded\",len(all_data),\"Entries\")\n",
    "    all_params = torch.from_numpy(np.array(all_params)).to(torch.float32).to(device)\n",
    "    return all_data, all_params, all_length\n",
    "\n",
    "\"\"\"\n",
    "# save the data in a torch file which is much faster to load then the .csvs\n",
    "\n",
    "data, params, length = load_data()\n",
    "data_dict = {\"data\": data,\n",
    "            \"params\": params,\n",
    "            \"length\": length}\n",
    "\n",
    "torch.save(data_dict, \"data_second.save\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(\"data_second.save\", map_location = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_dict[\"data\"][:290000]\n",
    "padded_data = pad(data)\n",
    "params = data_dict[\"params\"][:290000]\n",
    "length = data_dict[\"length\"][:290000]\n",
    "test_dat = data_dict[\"data\"][290000:]\n",
    "test_params = data_dict[\"params\"][290000:]\n",
    "test_length = data_dict[\"length\"][290000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor():\n",
    "    \"\"\"\n",
    "    Used to save initial means and range of the parameters and\n",
    "    applies the preprocessing\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        self.means = params.mean(dim = 0)\n",
    "        self.max = torch.abs(params-self.means).max(dim = 0)[0]+1e-5\n",
    " \n",
    "    def __call__(self, params, fwd = True):\n",
    "        if fwd:\n",
    "            return torch.arctanh((params - self.means)/self.max)\n",
    "        else:\n",
    "            return torch.tanh(params) * self.max + self.means\n",
    "\n",
    "prepper = preprocessor(params)\n",
    "prepped = prepper(params, fwd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder base class for use in an autoencoder\n",
    "    N: amount of linear layers\n",
    "    hidden_size: width of the hidden layers\n",
    "    out_size: output dim of the decoder, should match the rnns input dim\n",
    "    lr: learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, N, inp_size, hidden_size, out_size, lr = 1e-3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.inp_size = inp_size\n",
    "        self.linear = get_linear_subnet(N, inp_size, hidden_size, out_size)\n",
    "        self.params_trainable = list(filter(\n",
    "                lambda p: p.requires_grad, self.linear.parameters())) \n",
    "        n_trainable = sum(p.numel() for p in self.params_trainable)\n",
    "        print(f\"Number of Decoder parameters: {n_trainable}\", flush=True)        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "                self.params_trainable,\n",
    "                lr = lr,\n",
    "                betas =[0.9, 0.999],\n",
    "                eps = 1e-6,\n",
    "                weight_decay = 0\n",
    "            )\n",
    "        self.scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                verbose = True\n",
    "            )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class AE():\n",
    "    \"\"\"\n",
    "    Autoencoder class used to pretrain the RNN\n",
    "    as decoder a simple fcnn is used\n",
    "    rnn: object of type RNN which is trained\n",
    "    N: amount of linear layers for the decoder\n",
    "    hidden_size: width of the hidden decoder layers\n",
    "    dim: output dim of the decoder, should match the rnns input dim\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn, N = 6, hidden_size = 256, dim = 4):\n",
    "        self.dim = dim\n",
    "        self.encoder = rnn\n",
    "        self.decoder = Decoder(N, rnn.get_dim(), hidden_size, 149 * self.dim)\n",
    "    def train(self, epochs, x, lengths, batch_size = 2048):\n",
    "        loss_curve = []\n",
    "        metrics_curve = []\n",
    "        zeros = torch.zeros((batch_size,149,self.dim), dtype = torch.float32, device = device)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_index = np.random.permutation(len(x))\n",
    "            epoch_losses = 0\n",
    "            for i in range(len(x)//batch_size):\n",
    "                xsamps = x[epoch_index][i*batch_size:(i+1)*batch_size]\n",
    "                length = lengths[epoch_index][i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                packed_xsamps = pack(xsamps, length)\n",
    "                \n",
    "                self.decoder.optimizer.zero_grad(set_to_none=True)\n",
    "                self.encoder.optimizer.zero_grad(set_to_none=True)\n",
    "                encoded = self.encoder(packed_xsamps)\n",
    "                decoded = self.decoder(encoded).view(-1,149,self.dim)\n",
    "                \n",
    "                difference = (torch.where(xsamps != 0, decoded, zeros)-xsamps)**2\n",
    "                loss = torch.mean(difference)\n",
    "                if loss < 1e30:\n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    print(f\"loss is {loss}\")\n",
    "                    return\n",
    "                self.encoder.optimizer.step()\n",
    "                self.decoder.optimizer.step()\n",
    "                epoch_losses += loss.item()/(len(x)//batch_size)\n",
    "            loss_curve.append(epoch_losses)\n",
    "            self.encoder.scheduler.step(epoch_losses)\n",
    "            self.decoder.scheduler.step(epoch_losses)\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            print(\"Loss:\", epoch_losses)\n",
    "        self.decoder.optimizer.zero_grad(set_to_none=True)\n",
    "        self.encoder.optimizer.zero_grad(set_to_none=True)\n",
    "        plt.plot(np.arange(len(loss_curve)),np.array(loss_curve))      \n",
    "        \n",
    "    def inference(self, x):\n",
    "        with torch.no_grad():\n",
    "            decoded = self.decoder(self.encoder(x.view(1,-1,self.dim))).view(149,self.dim).cpu().numpy()\n",
    "        xnump = x.cpu().numpy()\n",
    "        fig, ax = plt.subplots(7, figsize = (8,16))\n",
    "        names = [\"Susceptible\", \"Infected\", \"Immune\", \"Dead\", \"Asymptomatic\", \"Hospitalized\", \"Severe\"]\n",
    "        for i in range(7):\n",
    "            ax[i].plot(np.arange(len(xnump)), xnump[:,i], label = f\"Truth\")\n",
    "            ax[i].plot(np.arange(149), decoded[:,i], label = f\"Fake\")\n",
    "            ax[i].set_ylabel(names[i])\n",
    "            \n",
    "            ax[i].legend()\n",
    "            ax[i].set_ylim([0,1])\n",
    "        ax[3].set_xlabel(\"Time in days\")\n",
    "        plt.savefig(\"rnn.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t7RUBOXouZ8L",
   "metadata": {
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1632071802957,
     "user": {
      "displayName": "Tobi Krebs",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10647988187130389750"
     },
     "user_tz": -120
    },
    "id": "t7RUBOXouZ8L"
   },
   "outputs": [],
   "source": [
    "def get_linear_subnet(N, inp_size, hidden_size, out_size):\n",
    "    \"\"\"\n",
    "    Helper function to get linear network with ReLU activation\n",
    "    N: amount of layers, not including the final linear output layer\n",
    "    inp_size: dimension of the input\n",
    "    hidden_size: width of the N-1 hidden layers\n",
    "    out_size: dimension of the output\n",
    "    \"\"\"\n",
    "    layer_list = []\n",
    "    layer_list.append(nn.Linear(inp_size, hidden_size))\n",
    "    layer_list.append(nn.ReLU())\n",
    "    for i in range(N-1):\n",
    "        layer_list.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layer_list.append(nn.ReLU())\n",
    "    layer_list.append(nn.Linear(hidden_size, out_size))\n",
    "    return nn.Sequential(*layer_list).to(device)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\"\n",
    "    RNN baseclass, used as conditioning network for the cINN. Implemented as an LSTM\n",
    "    inp_size: dimension of the input timeseries\n",
    "    hidden_size: parallel channels used by the rnns\n",
    "    numm_rnns: amount of lstms\n",
    "    lr: learning rate\n",
    "    bi: wether to use bidirectional lstms or not\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size, hidden_size = 5, num_rnns = 6, lr = 1e-3, bi = False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.inp_size = inp_size\n",
    "        self.rnn = nn.LSTM(inp_size, hidden_size, num_rnns, batch_first = True, bidirectional = bi).to(device)\n",
    "        self.params_trainable = list(filter(\n",
    "                lambda p: p.requires_grad, self.rnn.parameters())) \n",
    "        n_trainable = sum(p.numel() for p in self.params_trainable)\n",
    "        print(f\"Number of RNN parameters: {n_trainable}\", flush=True)        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "                self.params_trainable,\n",
    "                lr = lr,\n",
    "                betas =[0.9, 0.99],\n",
    "                eps = 1e-6,\n",
    "                weight_decay = 0\n",
    "            )\n",
    "        self.scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                verbose = True\n",
    "            )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        full, (last, cn) = self.rnn(x)\n",
    "        return torch.swapaxes(last, 0, 1).reshape(last.shape[1], -1)\n",
    "    \n",
    "    def get_dim(self):\n",
    "        \"\"\"\n",
    "        passes dummy variable through network to get the outputs shape\n",
    "        \"\"\"\n",
    "        return self.forward(torch.randn(1,100,self.inp_size, device = device)).shape[1]\n",
    "        \n",
    "    \n",
    "class cINN(nn.Module):\n",
    "    \"\"\"\n",
    "    cINN baseclass, using cubic spline blocks.\n",
    "    inp_size: dimension of the input\n",
    "    cond_size: dimension of the conditions\n",
    "    num_blocks: amount of coupling blocks used\n",
    "    sub_layers: amount of linear layers in the subnetworks\n",
    "    sub_width: width of the subnetworks\n",
    "    lr: learning rate, scheduler used is reduce_on_plateau\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_size, cond_size, num_blocks = 10, sub_layers = 3, sub_width = 256, lr = 2e-4):\n",
    "        super(cINN, self).__init__()\n",
    "        constructor_fct = lambda x_in, x_out: get_linear_subnet(sub_layers, \n",
    "                                                                x_in,\n",
    "                                                                x_in,\n",
    "                                                                x_out)\n",
    "\n",
    "        block_kwargs = {\n",
    "                        \"num_bins\": 120,\n",
    "                        \"subnet_constructor\": constructor_fct,\n",
    "                        \"bounds_init\": 5,\n",
    "                        \"permute_soft\": False\n",
    "                           }\n",
    "        inp_size = (inp_size,)        \n",
    "        nodes = [InputNode(*inp_size, name='inp')]\n",
    "        cond_node = ConditionNode(*(cond_size,))\n",
    "        for i in range(num_blocks):\n",
    "            nodes.append(Node(\n",
    "                    [nodes[-1].out0],\n",
    "                    CubicSplineBlock,\n",
    "                    block_kwargs,\n",
    "                    conditions = cond_node,\n",
    "                    name = f\"block_{i}\",\n",
    "                    \n",
    "                ))\n",
    "        nodes.append(OutputNode([nodes[-1].out0], name='out'))\n",
    "        nodes.append(cond_node)\n",
    "        self.model = GraphINN(nodes, verbose=False).to(device)\n",
    "        self.params_trainable = list(filter(\n",
    "                lambda p: p.requires_grad, self.model.parameters()))\n",
    "        n_trainable = sum(p.numel() for p in self.params_trainable)\n",
    "        print(f\"Number of cINN parameters: {n_trainable}\", flush=True)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "                self.params_trainable,\n",
    "                lr = lr,\n",
    "                betas =[0.9, 0.99],\n",
    "                eps = 1e-6,\n",
    "                weight_decay = 0\n",
    "            )\n",
    "        self.scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                verbose = True\n",
    "            )\n",
    "    def forward(self, x, cond = None, rev = False):\n",
    "        return self.model(x, c = cond, rev = rev)\n",
    "\n",
    "\n",
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Wrapper for training cINN and RNN at the same time, also used for inference\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn, cinn):\n",
    "        self.cinn = cinn\n",
    "        self.rnn = rnn\n",
    "\n",
    "    def metrik(self, true, data, zeros):\n",
    "        \"\"\"\n",
    "        Used to measure the distance between truth and generated parameters\n",
    "        Better interpretable than the Loss function\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            cond = self.rnn(data)\n",
    "            output, _ = self.cinn(zeros, cond, rev = True)\n",
    "            output = torch.abs(prepper(output, fwd = False) - prepper(true, fwd = False))\n",
    "            output = torch.mean(output, dim = 0).cpu().numpy()\n",
    "        print(\"Metric:\", output)\n",
    "        return output\n",
    "    \n",
    "    def train(self, epochs, xtrain, ytrain, lengths, batch_size, train_rnn = True):\n",
    "        \"\"\"\n",
    "        Trains the cINN and shows loss and metric plots\n",
    "        epochs: amount of epochs to train\n",
    "        xtrain: training parameters\n",
    "        ytrain: corresponding time series, need to be zero padded\n",
    "        lengths: real lengths of the time series, used for packing them again\n",
    "        batch_size: Batch size used for the training\n",
    "        train_rnn: Enable weight updates for the RNN\n",
    "        \"\"\"\n",
    "        loss_curve = []\n",
    "        metrics_curve = []\n",
    "        zeros = torch.zeros(batch_size,5).to(device)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_index = np.random.permutation(len(xtrain))\n",
    "            epoch_losses = 0\n",
    "            for i in range(len(xtrain)//batch_size):\n",
    "                ysamps = ytrain[epoch_index][i*batch_size:(i+1)*batch_size]\n",
    "                xsamps = xtrain[epoch_index][i*batch_size:(i+1)*batch_size]\n",
    "                length = lengths[epoch_index][i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                packed_ysamps = pack(ysamps, length)\n",
    "                \n",
    "                self.cinn.optimizer.zero_grad(set_to_none=True)\n",
    "                if train_rnn:\n",
    "                    self.rnn.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                cond = self.rnn(packed_ysamps)\n",
    "\n",
    "                gauss, jac = self.cinn(xsamps, cond)\n",
    "                loss = torch.mean(gauss**2/2) - torch.mean(jac)/gauss.shape[1]\n",
    "                    \n",
    "                loss.backward()\n",
    "                self.cinn.optimizer.step()\n",
    "                if train_rnn:\n",
    "                    self.rnn.optimizer.step()\n",
    "                epoch_losses += loss.item()/(len(xtrain)//batch_size)\n",
    "            loss_curve.append(epoch_losses)\n",
    "            \n",
    "            self.cinn.scheduler.step(epoch_losses)\n",
    "            if train_rnn:\n",
    "                self.rnn.scheduler.step(epoch_losses)\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            print(\"Loss:\", epoch_losses)\n",
    "            metrics_curve.append(self.metrik(xsamps,packed_ysamps, zeros))\n",
    "        plt.plot(np.arange(len(loss_curve)),np.array(loss_curve), label = \"Loss\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "        metrics_curve = np.array(metrics_curve)\n",
    "        for i in range(metrics_curve.shape[1]):\n",
    "            plt.plot(np.arange(len(loss_curve)), metrics_curve[:,i], label = f\"Metric $x_{i}$\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Metric\")\n",
    "        \n",
    "    def inference(self, data_point, true_param, twod = False):\n",
    "        \"\"\"\n",
    "        Predicts a parameter distribution for given time series\n",
    "        Overlays the true parameter to see how well the network performs\n",
    "        twod: Addionally plot 2d correlations\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(100):\n",
    "                gauss = torch.randn(1000,5).to(device)\n",
    "                cond = self.rnn(data_point.repeat(1000,1,1).to(device))\n",
    "                output, _ = self.cinn(gauss, cond, rev = True)\n",
    "                outputs.append(output)\n",
    "        output = torch.cat(outputs, dim = 0)\n",
    "        output = prepper(output, fwd = False).detach().cpu()\n",
    "        names = [\"Initial infected\", \"Initial immune\", \"Mobility susceptible\", \"Mobility infected\", \"Mobility immune\"]\n",
    "        fig, axis = plt.subplots(3,2, figsize = (10,12))\n",
    "        for i in range(len(names)):\n",
    "            ax = axis[int((i-i%2)/2),i%2]\n",
    "            ax.hist(output.numpy()[:,i], bins = 100, density = True, label = \"Generated\")\n",
    "            ax.axvline(true_param[i], color = \"r\", label = \"Truth\")\n",
    "            ax.set_xlabel(names[i], fontsize = 12)\n",
    "            ax.legend()\n",
    "            if i%2 == 0:\n",
    "                ax.set_ylabel(\"Normalized\", fontsize = 12)\n",
    "        plt.show()\n",
    "        if twod:\n",
    "            fig, axis = plt.subplots(2,2, figsize = (10,12))\n",
    "            for i in range(3):\n",
    "                ax = axis[int((i-i%2)/2),i%2]\n",
    "                hist, xedges, yedges = np.histogram2d(output.numpy()[:,2+i], output.numpy()[:,2+(i+1)%3], bins=[100,100], density=True)\n",
    "                mapp = ax.pcolormesh(xedges, yedges, hist.T, rasterized=True)\n",
    "                ax.set_xlabel(names[2+i])\n",
    "                ax.set_ylabel(names[2+(i+1)%3])\n",
    "            fig.colorbar(mapp, cax = axis[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-norwegian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(7, hidden_size = 10, num_rnns = 6, lr = 1e-3, bi = False)\n",
    "ae = AE(rnn, dim = 7)\n",
    "ae.train(10, padded_data, np.array(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.inference(test_dat[np.random.randint(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EK1Xzxo0uZ8N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 218418,
     "status": "error",
     "timestamp": 1632072083696,
     "user": {
      "displayName": "Tobi Krebs",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10647988187130389750"
     },
     "user_tz": -120
    },
    "id": "EK1Xzxo0uZ8N",
    "outputId": "cc10d333-9b19-41bb-ee58-9d560eb166f8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cinn = cINN(5, rnn.get_dim())\n",
    "network = Estimator(rnn, cinn)\n",
    "network.train(10, prepped, padded_data, np.array(length), batch_size = 1024, train_rnn = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(200, prepped, padded_data, np.array(length), batch_size = 1024, train_rnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-introduction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(len(test_dat))\n",
    "network.inference(test_dat[i], test_params[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(test_dat))\n",
    "network.inference(test_dat[i], test_params[i], twod = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "encoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
